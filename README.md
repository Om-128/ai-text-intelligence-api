# AI Text Intelligence API ğŸš€

**FastAPI-based backend** for **sentiment analysis**, **text summarization**, **semantic text ingestion**, and **Retrieval-Augmented Generation (RAG)** using a **local Large Language Model**.

[![Python](https://img.shields.io/badge/Python-3.11-blue.svg)](https://www.python.org/)
[![FastAPI](https://img.shields.io/badge/FastAPI-0.104-brightgreen.svg)](https://fastapi.tiangolo.com/)
[![Docker](https://img.shields.io/badge/Docker-ready-blue.svg)](https://www.docker.com/)

## Features

- ğŸ” **Plain text ingestion** for semantic indexing
- ğŸ—„ï¸ **Vector storage** using ChromaDB
- ğŸ¤— **HuggingFace sentence-transformer embeddings**
- ğŸ¯ **Source-aware RAG** to prevent hallucination
- ğŸ§  **Local LLM inference** using Ollama (Gemma 2B)
- âš ï¸ **Graceful degradation** when LLM unavailable
- ğŸ—ï¸ **Clean service-based architecture**
- ğŸ’¾ **Persistent vector database**
- ğŸ³ **Docker-ready deployment**

## ğŸ›  Tech Stack

| Component | Technology |
|-----------|------------|
| **Framework** | Python 3.11, FastAPI |
| **Embeddings** | HuggingFace Sentence Transformers |
| **Vector DB** | ChromaDB |
| **LLM** | Ollama (Gemma 2B) |
| **Runtime** | ONNX Runtime (pinned) |
| **Orchestration** | LangChain |
| **Container** | Docker |

## ğŸš€ Quick Start

### 1. Install & Setup Ollama
```
Download from https://ollama.com/

ollama pull gemma:2b<br>
```

### 2. Clone & Setup
```
git clone https://github.com/Om-128/ai-text-intelligence-api.git

cd ai-text-intelligence-api
``` 
### 3. Virtual Environment
```
python -m venv test_env

Activate On Windows:
test_env\Scripts\activate

Activate On Linux/Mac:
source test_env/bin/activate

```

### 4. Install Dependencies
```
pip install -r requirements.txt
```

### 5. Run the FastAPI
```
python -m uvicorn app.main:app --reload
```

### 6. API Usage

```
Text Analysis
  - POST /analyze <br>
  - add Content-Type: json

Text Summarization
  - POST /summarize <br>
  - add Content-Type: text/plain

Semantic Text Ingestion
  - POST /semantic_add/ <br>
  - add Content-Type: text/plain

Semantic RAG Query
  - POST /semantic_search/ <br>
  - add Content-Type: text/plain
```

### ğŸ³ Docker Support
### Build
docker build -t ai-text-intelligence-api .

### Run (Ollama must be running on host)
docker run -p 8000:8000 ai-text-intelligence-api


### ğŸ“ Project Structure
text
```
ai-text-intelligence-api/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ api/
â”‚   â”‚   â””â”€â”€ analyze.py         # Sentiment analysis and keywords API
â”‚   â”‚   â””â”€â”€ summarize.py       # Text summarization API
â”‚   â”‚   â”œâ”€â”€ semantic_add.py    # Ingestion API
â”‚   â”‚   â””â”€â”€ semantic_rag.py    # RAG API
â”‚   â”œâ”€â”€ services/
â”‚   â”‚   â””â”€â”€ semantic_search/
â”‚   â”‚       â”œâ”€â”€ embeddings.py     # Embedding model
â”‚   â”‚       â”œâ”€â”€ ingest.py         # Ingestion logic
â”‚   â”‚       â””â”€â”€ rag.py            # RAG logic
â”‚   â”‚   â””â”€â”€ sentiment_service.py  # Sentiment analysis
â”‚   â”‚   â””â”€â”€ keyword_service.py    # Top keywords logic
â”‚   â”‚   â””â”€â”€ summarization_service # Text summarization logic
â”‚   â”œâ”€â”€ utils.py
â”‚   â”œâ”€â”€ CustomException.py
â”‚   â””â”€â”€ main.py
â”œâ”€â”€ vectorstore/
â”‚   â””â”€â”€ chroma_db/             # Persistent storage
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ Dockerfile
â””â”€â”€ README.md
```

ğŸ” Notes <br>
#### Automatic DB Creation: Vector database and collections created on first use

#### LLM Fallback: Semantic retrieval works even if Ollama unavailable

#### Local Only: Designed for experimentation and evaluation

#### Pinned Dependencies: All native libraries version-locked for stability